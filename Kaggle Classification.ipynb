{
 "metadata": {
  "name": "",
  "signature": "sha256:f9dc33e119db2430b9c8f0904c40389bce197fc667539af716ae959660cc7a6f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Kaggle Classification\n",
      "Our final algorithm is to use SVM as classifier and some features such as number of nouns, verbs, adjectives and verbs, number of unique words, number of year and urls and the similarity between words and predefiend categories using WordNet. We also used cross validatoin to test the score.\n",
      "\n",
      "We tried simple features like length of sentences or length of words, but the score didn't get improved. The score was improved after we tried other features but it didn't help us to get a better results. We also found out that SVM did a better job than Na\u00efve Bayes classifier using k-folds cross validation and that's why we use SVM in the end. \n",
      "\n",
      "Matt tried SVM and features like number of nouns, verbs, adjectives and verbs, number of unique words, number of year and urls. Pi-Tan tried Na\u00efve Bayes classifier and some features like length of sentences, length of words, the similarity between words and predefiend categories and hypernyms. She also tried use k-folds cross validatoin to test between different classfiers and features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, string, collections, csv, codecs, numpy\n",
      "from nltk.corpus import wordnet as wn\n",
      "from sklearn import svm, cross_validation\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "# filepath = '/Users/matthewvalente/Documents/ischool/fall14/Info256_NLP/Kaggle-NLP-Assignment/'\n",
      "filepath = ''\n",
      "train_file = 'train.txt'\n",
      "test_file = 'test.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###File manipulation\n",
      "read txt file and write output result"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_in_text(filepath, filename, csv_file=False):\n",
      "    with codecs.open(filepath + filename, encoding='utf-8') as f:\n",
      "        if csv_file == False:\n",
      "            tups = [(line[0], line[2:].strip()) for line in f]\n",
      "        else:\n",
      "            reader = csv.reader(f)\n",
      "            next(reader, None)  # skip header\n",
      "            tups = [tuple(row) for row in reader]\n",
      "    y, X = [list(t) for t in zip(*tups)]\n",
      "    return X, y         # for test.csv file, y is IDs not class values\n",
      "\n",
      "\n",
      "def output(y_pred, IDs):\n",
      "    tups = zip(IDs, y_pred)\n",
      "    tups.insert(0, (\"ID\", \"Category\"))\n",
      "    with open(\"output.csv\", \"w\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        for tup in tups:\n",
      "            writer.writerow(tup)\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "# def hypernyms(untokenized):\n",
      "#     sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()\n",
      "#     stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "#     punct = string.punctuation                                      # list of punctuation to be removed\n",
      "#     mappings = [\"URL\", \"YEAR\"]\n",
      "#     hyps = []           \n",
      "#     wnl = nltk.WordNetLemmatizer()\n",
      "#     for question in sents:\n",
      "#         hyps.append([])     # new list for each question\n",
      "#         for sent in question:\n",
      "#             tokens = nltk.word_tokenize(sent)\n",
      "#             normal = []\n",
      "#             for word in tokens:\n",
      "#                 if word not in stopwords and word not in punct:\n",
      "#                     if word in mappings:\n",
      "#                         normal.append(word)    # don't lemmatize the mappings!\n",
      "#                     else:\n",
      "#                         normal.append(wnl.lemmatize(word))\n",
      "#             #if len(normal) > 0:\n",
      "#             for word in normal:\n",
      "#                 if word in mappings:    # don't hypernym mappings!\n",
      "#                     hyps[-1].append(word)\n",
      "#                 else:\n",
      "#                     terms = []\n",
      "#                     s = wn.synsets(word, 'n')\n",
      "#                     for syn in s:\n",
      "#                         for h in syn.hypernyms():\n",
      "#                             terms = terms + [h.name]\n",
      "#                         hyps[-1].extend(terms)\n",
      "#     return hyps\n",
      "\n",
      "# def cross_validate(X, y):\n",
      "#     strings = [' '.join(x) for x in X]\n",
      "#     vectorizer = CountVectorizer(analyzer=str.split)\n",
      "#     X = vectorizer.fit_transform(strings)\n",
      "#     clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "#     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "#     random_state=None, tol=0.0001, verbose=0) \n",
      "#     X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "#     clf.fit(X_train, y_train)\n",
      "#     #for x in X_test:\n",
      "#     #    print clf.predict(x)\n",
      "#     return clf.score(X_test, y_test) \n",
      "\n",
      "# def predict(filepath, filename, classifier, vectorizer):\n",
      "#     strings, IDs = read_in_text(filepath, filename, csv_file=True)\n",
      "#     new_strings = mappings(strings)\n",
      "#     hyps = hypernyms(new_strings)\n",
      "#     s = [' '.join(question) for question in hyps]\n",
      "#     X = vectorizer.transform(s)   # use transform instead of fit_transform to ensure matching dimensionality\n",
      "#     y_pred = [classifier.predict(x)[0] for x in X]\n",
      "#     return y_pred, IDs\n",
      "        \n",
      "# def classify_hypernyms_svm(hypernyms, y):\n",
      "#     strings = [' '.join(question) for question in hypernyms]\n",
      "#     vectorizer = CountVectorizer(analyzer=str.split)\n",
      "#     X = vectorizer.fit_transform(strings)\n",
      "#     clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "#     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "#     random_state=None, tol=0.0001, verbose=0) \n",
      "#     clf.fit(X,y)\n",
      "#     return clf, vectorizer\n",
      "\n",
      "# def train_algorithm1(filepath, filename):\n",
      "#     strings, y = read_in_text(filepath, filename)\n",
      "#     new_strings = mappings(strings)\n",
      "#     hyps = hypernyms(new_strings)\n",
      "#     classifier, vectorizer = classify_hypernyms_svm(hyps, y)\n",
      "#     print cross_validate(hyps,y)\n",
      "#     return classifier, vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Text normalization and tokenization\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remove weird format((e.g. \"&#xd;&lt;br&gt;\")) from raw data\n",
      "# Replace real url and year to words URL and YEAR\n",
      "def mappings(untokenized):\n",
      "    '''Perform on list of text strings'''\n",
      "    x = [re.sub(r\"&#(\\S*?;)+\", \" \", question) for question in untokenized] # strips weird formatting characters (e.g. \"&#xd;&lt;br&gt;\")\n",
      "    x = [re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?\", \" URL \", question) for question in x] # map URLs to \"URL\"\n",
      "    x = [re.sub(r\"[12]\\d{3}\", \" YEAR \", question) for question in x]    # map years to \"YEAR\"\n",
      "    x = [re.sub(r\"\\d+\", \" \", question) for question in x]\n",
      "    return x\n",
      "\n",
      "# Remove punctuation and stopwords and then tokenize the data \n",
      "def bag_tokens(untokenized):\n",
      "    tokens = []\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()\n",
      "    stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "    punct = string.punctuation                                      # list of punctuation to be removed\n",
      "    for question in sents:\n",
      "        tokens.append([])     # new list for each question\n",
      "        for sent in question:\n",
      "            words = nltk.word_tokenize(sent)\n",
      "            for word in words:\n",
      "                if word not in stopwords and word not in punct:\n",
      "                    tokens[-1].append(word.lower())\n",
      "    return tokens"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Feature extraction\n",
      "1. Number of nouns, verbs, adjectives and verbs\n",
      "2. Number of unique words\n",
      "3. Number of year and urls\n",
      "4. the similarity between words and predefiend categories using WordNet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Feature: Number of nouns, verbs, adjectives and verbs\n",
      "\n",
      "def pos_tags(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of pos tagged tokens\n",
      "        Each question is a list of pos tagged sentences\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]    # list of sentences; used as input for word_tokenize()\n",
      "    punct = string.punctuation    # list of punctuation to be removed\n",
      "    tags = []\n",
      "    for question in sents:\n",
      "        tags.append([])\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            tags[-1].append(nltk.pos_tag(tokens))\n",
      "    return tags\n",
      "\n",
      "           \n",
      "# def extract_pos_features(pos_tags):\n",
      "#     features = []\n",
      "#     for question in pos_tags:\n",
      "#         bag = []    # list of pos tags for the question\n",
      "#         for sentence in question:\n",
      "#             for (word, pos) in sentence:\n",
      "#                 bag.append(pos)\n",
      "#         features.append(bag)\n",
      "#     return features\n",
      "\n",
      "def extract(filepath, filename):\n",
      "    X, y = read_in_text(filepath, filename)\n",
      "    mapped = mappings(X)\n",
      "    pos_tokens = pos_tags(mapped)\n",
      "    features = []\n",
      "    # count nouns, verbs, adjectives\n",
      "    for question in pos_tokens:\n",
      "        vector = [1,1,1,1]\n",
      "        for sentence in question:\n",
      "            for (word, pos) in sentence:\n",
      "                if re.match(r\"^N.*\", pos):    # match noun\n",
      "                    vector[0] += 1\n",
      "                if re.match(r\"^V.*\", pos):    # match verb\n",
      "                    vector[1] += 1\n",
      "                if re.match(r\"^J.*\", pos):    # match adjective\n",
      "                    vector[2] += 1\n",
      "                if re.match(r\"^R.*\", pos):    # match adverb\n",
      "                    vector[3] += 1\n",
      "        features.append(map(lambda x: x/4, vector))\n",
      "    return features\n",
      "\n",
      "\n",
      "# Feature: Number of unique words\n",
      "# Part 1: Calculate the fequencies of the words\n",
      "def categories(X, y):\n",
      "    combo = zip(X, y)\n",
      "    categories_dic = {}\n",
      "    for i in range(1,8):\n",
      "        categories_dic[str(i)] = [x for (x,y) in combo if y == str(i)]\n",
      "    return categories_dic\n",
      "\n",
      "def category_word_frequencies(X, y):\n",
      "    category_dic = categories(X, y)\n",
      "    freq = []\n",
      "    combo = []\n",
      "    unique = {}\n",
      "    for key, value in category_dic.iteritems():\n",
      "        tokens = []\n",
      "        for b in bag_tokens(mappings(value)):\n",
      "            tokens.extend(b)\n",
      "        dist = nltk.FreqDist(tokens)\n",
      "        freq.append((key, dist.keys()[:100]))\n",
      "        combo.extend(dist.keys()[:100])\n",
      "    combo_dist = nltk.FreqDist(combo)\n",
      "    to_remove = [word for word, frequency in combo_dist.items() if frequency > 1]\n",
      "    for category, word_list in freq:\n",
      "        u = []\n",
      "        for word in word_list:\n",
      "            if word not in to_remove:\n",
      "                u.append(word)\n",
      "        unique[category] = u\n",
      "    return unique\n",
      "\n",
      "# Feature: Number of unique words\n",
      "# Part 1: Calculate the number of unique words\n",
      "def count_uniques(bag_tokens, uniques):\n",
      "    keys_sorted = sorted(uniques.keys())\n",
      "    features = []\n",
      "    for question in bag_tokens:\n",
      "        vector = [1,1,1,1,1,1,1]\n",
      "        for word in question:\n",
      "            index = 0\n",
      "            for key in keys_sorted:\n",
      "                if word in uniques[key]:\n",
      "                    vector[index] += 1\n",
      "                index += 1\n",
      "        features.append(vector)\n",
      "    return features\n",
      "\n",
      "# Feature: Number of year and url\n",
      "def count_mappings(bag_tokens, map_list):\n",
      "    features = []\n",
      "    for question in bag_tokens:\n",
      "        vector = [1,1]\n",
      "        for word in question:\n",
      "            index = 0\n",
      "            for m in map_list:\n",
      "                if word == m:\n",
      "                    vector[index] += 1\n",
      "                index += 1\n",
      "        features.append(vector)\n",
      "    return features\n",
      "\n",
      "# Feature: Compare the similariy between a word with predefined categories using WordNet.\n",
      "def word_similarity(data):\n",
      "    similarity_features = []   \n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    stopwords = nltk.corpus.stopwords.words('english') \n",
      "    predefined_groups = [['Business', 'Finance'], ['Computers', 'Internet'], \\\n",
      "                         ['Entertainment', 'Music'], ['Family', 'Relationships'],\\\n",
      "                         ['Education', 'Reference'], ['Health'], ['Science', 'Mathematics']]\n",
      "    \n",
      "    for idx, question in enumerate(data):\n",
      "        strings = ''.join(mappings(question.lower()))\n",
      "        words = [wnl.lemmatize(word) for word in nltk.word_tokenize(strings) if word not in stopwords and word not in string.punctuation]\n",
      "        similarity = []\n",
      "        for groups in predefined_groups:\n",
      "            group_similarity = []\n",
      "            for word in words:\n",
      "                word_syns = wn.synsets(word, 'n')\n",
      "                for group in groups:\n",
      "                    group_syns = wn.synsets(group.lower(), 'n')\n",
      "                    if len(word_syns) > 0 and len(group_syns) > 0:\n",
      "                        similarity_value = word_syns[0].path_similarity(group_syns[0])\n",
      "                        if numpy.isnan(similarity_value):\n",
      "                            group_similarity.append(0)\n",
      "                        else:\n",
      "                            group_similarity.append(similarity_value)\n",
      "            if numpy.isnan(numpy.mean(group_similarity)):\n",
      "                similarity.append(0)\n",
      "            else:\n",
      "                similarity.append(numpy.mean(group_similarity))\n",
      "        similarity_features.append(similarity)\n",
      "        \n",
      "    return similarity_features \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Final algorithm and test score"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def algorithm2(filepath, filename):\n",
      "    X, y = read_in_text(filepath, filename)\n",
      "    m = mappings(X)\n",
      "    tokens = bag_tokens(m)\n",
      "    pos_features = extract(filepath, filename)    # count of nouns, verbs, adjectives\n",
      "    unique_dic = category_word_frequencies(X, y)\n",
      "    uniques_features = count_uniques(tokens, unique_dic)    # count of a dictionary of unique terms for each category\n",
      "    map_features = count_mappings(tokens, [\"year\", \"url\"])    # count of urls and dates\n",
      "    similarity_features = word_similarity(X)    # similarity between words and predefined categories\n",
      "    combo_features = [pos_features[i] + uniques_features[i] + map_features[i] + similarity_features[i] for i in range(len(y))]\n",
      "    return combo_features, y\n",
      "\n",
      "\n",
      "combo, y = algorithm2(filepath, train_file)\n",
      "clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "random_state=None, tol=0.0001, verbose=0) \n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(combo, y, test_size=0.2, random_state=0)\n",
      "clf.fit(X_train, y_train)\n",
      "print clf.score(X_test, y_test) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.401851851852\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}