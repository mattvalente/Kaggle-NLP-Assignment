{
 "metadata": {
  "name": "",
  "signature": "sha256:957f23afa541be48c50d81f8e106f797fa2a2c025e1c69bb390219d8255e16a5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, string, collections, csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "from sklearn import svm, cross_validation\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "filepath = '/Users/matthewvalente/Documents/ischool/fall14/Info256_NLP/Kaggle-NLP-Assignment/'\n",
      "train_file = 'train.txt'\n",
      "test_file = 'test.csv'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_in_text(filepath, filename, csv_file=False):\n",
      "    with open(filepath + filename) as f:\n",
      "        if csv_file == False:\n",
      "            tups = [(line[0], line[2:].strip()) for line in f]\n",
      "        else:\n",
      "            reader = csv.reader(f)\n",
      "            next(reader, None)  # skip header\n",
      "            tups = [tuple(row) for row in reader]\n",
      "    y, X = [list(t) for t in zip(*tups)]\n",
      "    return X, y         # for test.csv file, y is IDs not class values\n",
      "\n",
      "\n",
      "def mappings(untokenized):\n",
      "    '''Perform on list of text strings'''\n",
      "    x = [re.sub(r\"&#(\\S*?;)+\", \" \", question) for question in untokenized] # strips weird formatting characters (e.g. \"&#xd;&lt;br&gt;\")\n",
      "    x = [re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?\", \" URL \", question) for question in x] # map URLs to \"URL\"\n",
      "    x = [re.sub(r\"[12]\\d{3}\", \" YEAR \", question) for question in x]    # map years to \"YEAR\"\n",
      "    x = [re.sub(r\"\\d+\", \" \", question) for question in x]\n",
      "    return x\n",
      "\n",
      "def bag_tokens(untokenized):\n",
      "    tokens = []\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()\n",
      "    stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "    punct = string.punctuation                                      # list of punctuation to be removed\n",
      "    for question in sents:\n",
      "        tokens.append([])     # new list for each question\n",
      "        for sent in question:\n",
      "            words = nltk.word_tokenize(sent)\n",
      "            for word in words:\n",
      "                if word not in stopwords and word not in punct:\n",
      "                    tokens[-1].append(word.lower())\n",
      "    return tokens\n",
      "\n",
      "def hypernyms(untokenized):\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()\n",
      "    stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "    punct = string.punctuation                                      # list of punctuation to be removed\n",
      "    mappings = [\"URL\", \"YEAR\"]\n",
      "    hyps = []           \n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    for question in sents:\n",
      "        hyps.append([])     # new list for each question\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            normal = []\n",
      "            for word in tokens:\n",
      "                if word not in stopwords and word not in punct:\n",
      "                    if word in mappings:\n",
      "                        normal.append(word)    # don't lemmatize the mappings!\n",
      "                    else:\n",
      "                        normal.append(wnl.lemmatize(word))\n",
      "            #if len(normal) > 0:\n",
      "            for word in normal:\n",
      "                if word in mappings:    # don't hypernym mappings!\n",
      "                    hyps[-1].append(word)\n",
      "                else:\n",
      "                    terms = []\n",
      "                    s = wn.synsets(word, 'n')\n",
      "                    for syn in s:\n",
      "                        for h in syn.hypernyms():\n",
      "                            terms = terms + [h.name]\n",
      "                        hyps[-1].extend(terms)\n",
      "    return hyps\n",
      "\n",
      "\n",
      "def pos_tags(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of pos tagged tokens\n",
      "        Each question is a list of pos tagged sentences\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]    # list of sentences; used as input for word_tokenize()\n",
      "    punct = string.punctuation    # list of punctuation to be removed\n",
      "    tags = []\n",
      "    for question in sents:\n",
      "        tags.append([])\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            tags[-1].append(nltk.pos_tag(tokens))\n",
      "    return tags\n",
      "\n",
      "\n",
      "\n",
      "def cross_validate(X, y):\n",
      "    strings = [' '.join(x) for x in X]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "    clf.fit(X_train, y_train)\n",
      "    #for x in X_test:\n",
      "    #    print clf.predict(x)\n",
      "    return clf.score(X_test, y_test) \n",
      "\n",
      "def predict(filepath, filename, classifier, vectorizer):\n",
      "    strings, IDs = read_in_text(filepath, filename, csv_file=True)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    s = [' '.join(question) for question in hyps]\n",
      "    X = vectorizer.transform(s)   # use transform instead of fit_transform to ensure matching dimensionality\n",
      "    y_pred = [classifier.predict(x)[0] for x in X]\n",
      "    return y_pred, IDs\n",
      "        \n",
      "\n",
      "\n",
      "def output(y_pred, IDs):\n",
      "    tups = zip(IDs, y_pred)\n",
      "    tups.insert(0, (\"ID\", \"Category\"))\n",
      "    with open(\"output.csv\", \"w\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        for tup in tups:\n",
      "            writer.writerow(tup)\n",
      "            \n",
      "def extract_pos_features(pos_tags):\n",
      "    features = []\n",
      "    for question in pos_tags:\n",
      "        bag = []    # list of pos tags for the question\n",
      "        for sentence in question:\n",
      "            for (word, pos) in sentence:\n",
      "                bag.append(pos)\n",
      "        features.append(bag)\n",
      "    return features\n",
      "\n",
      "def categories(X, y):\n",
      "    combo = zip(X, y)\n",
      "    categories_dic = {}\n",
      "    for i in range(1,8):\n",
      "        categories_dic[str(i)] = [x for (x,y) in combo if y == str(i)]\n",
      "    return categories_dic\n",
      "\n",
      "def extract(filepath, filename):\n",
      "    X, y = read_in_text(filepath, filename)\n",
      "    mapped = mappings(X)\n",
      "    pos_tokens = pos_tags(mapped)\n",
      "    features = []\n",
      "    # count nouns, verbs, adjectives\n",
      "    for question in pos_tokens:\n",
      "        vector = [1,1,1,1]\n",
      "        for sentence in question:\n",
      "            for (word, pos) in sentence:\n",
      "                if re.match(r\"^N.*\", pos):    # match noun\n",
      "                    vector[0] += 1\n",
      "                if re.match(r\"^V.*\", pos):    # match verb\n",
      "                    vector[1] += 1\n",
      "                if re.match(r\"^J.*\", pos):    # match adjective\n",
      "                    vector[2] += 1\n",
      "                if re.match(r\"^R.*\", pos):    # match adverb\n",
      "                    vector[3] += 1\n",
      "        features.append(map(lambda x: x/4, vector))\n",
      "    return features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_hypernyms_svm(hypernyms, y):\n",
      "    strings = [' '.join(question) for question in hypernyms]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    clf.fit(X,y)\n",
      "    return clf, vectorizer\n",
      "\n",
      "def train_algorithm1(filepath, filename):\n",
      "    strings, y = read_in_text(filepath, filename)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    classifier, vectorizer = classify_hypernyms_svm(hyps, y)\n",
      "    print cross_validate(hyps,y)\n",
      "    return classifier, vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "def category_word_frequencies(X, y):\n",
      "    category_dic = categories(X, y)\n",
      "    freq = []\n",
      "    combo = []\n",
      "    unique = {}\n",
      "    for key, value in category_dic.iteritems():\n",
      "        tokens = []\n",
      "        for b in bag_tokens(mappings(value)):\n",
      "            tokens.extend(b)\n",
      "        dist = nltk.FreqDist(tokens)\n",
      "        freq.append((key, dist.keys()[:100]))\n",
      "        combo.extend(dist.keys()[:100])\n",
      "    combo_dist = nltk.FreqDist(combo)\n",
      "    to_remove = [word for word, frequency in combo_dist.items() if frequency > 1]\n",
      "    for category, word_list in freq:\n",
      "        u = []\n",
      "        for word in word_list:\n",
      "            if word not in to_remove:\n",
      "                u.append(word)\n",
      "        unique[category] = u\n",
      "    return unique\n",
      "\n",
      "def count_uniques(bag_tokens, uniques):\n",
      "    keys_sorted = sorted(uniques.keys())\n",
      "    features = []\n",
      "    for question in bag_tokens:\n",
      "        vector = [1,1,1,1,1,1,1]\n",
      "        for word in question:\n",
      "            index = 0\n",
      "            for key in keys_sorted:\n",
      "                if word in uniques[key]:\n",
      "                    vector[index] += 1\n",
      "                index += 1\n",
      "        features.append(vector)\n",
      "    return features\n",
      "\n",
      "def count_mappings(bag_tokens, map_list):\n",
      "    features = []\n",
      "    for question in bag_tokens:\n",
      "        vector = [1,1]\n",
      "        for word in question:\n",
      "            index = 0\n",
      "            for m in map_list:\n",
      "                if word == m:\n",
      "                    vector[index] += 1\n",
      "                index += 1\n",
      "        features.append(vector)\n",
      "    return features\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = read_in_text(filepath, train_file)\n",
      "m = mappings(X)\n",
      "bag = bag_tokens(m)\n",
      "category_word_frequencies(X, y)\n",
      "#count_mappings(bag, [\"year\", \"url\"])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def algorithm2(filepath, filename):\n",
      "    X, y = read_in_text(filepath, filename)\n",
      "    m = mappings(X)\n",
      "    tokens = bag_tokens(m)\n",
      "    pos_features = extract(filepath, filename)    # count of nouns, verbs, adjectives\n",
      "    unique_dic = category_word_frequencies(X, y)\n",
      "    uniques_features = count_uniques(tokens, unique_dic)    # count of a dictionary of unique terms for each category\n",
      "    map_features = count_mappings(tokens, [\"year\", \"url\"])    # count of urls and dates\n",
      "    combo_features = [pos_features[i] + uniques_features[i] + map_features[i] for i in range(len(y))]\n",
      "    return combo_features\n",
      "\n",
      "#combo = algorithm2(filepath, train_file)\n",
      "#clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "#intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "#random_state=None, tol=0.0001, verbose=0) \n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(combo, y, test_size=0.2, random_state=0)\n",
      "clf.fit(X_train, y_train)\n",
      "print clf.score(X_test, y_test) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.542592592593\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
