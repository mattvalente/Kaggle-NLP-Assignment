{
 "metadata": {
  "name": "",
  "signature": "sha256:6fabad971f73c02c828afe3f29b1971106a1d466f14a582113d6ed8b892ed0f9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, collections\n",
      "import string\n",
      "#import enchant"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Load Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_path = '/Users/matthewvalente/Documents/ischool/fall14/Info256_NLP/Kaggle/'    # directory where kaggle data set is located\n",
      "file_name = 'train.txt'\n",
      "\n",
      "with open(file_path + file_name) as f:\n",
      "    train_tuples = [(line[0], line[2:].strip()) for line in f]\n",
      "\n",
      "Y = [tup[0] for tup in train_tuples]\n",
      "X_strings = [tup[1] for tup in train_tuples]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tokenize\n",
      "- And removed stopwords/punctuation/strange formatting (easier to remove in this step)\n",
      "- And map: urls to \"URL\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_strings = [re.sub(r\"&#(\\S*?;)+\", \" \", question) for question in X_strings] # strips weird formatting characters (e.g. \"&#xd;&lt;br&gt;\")\n",
      "X_strings = [re.sub(r\"(http:)?(www.)?\\w*?(.com|.gov|.edu)\", \" URL \", question) for question in X_strings] # map URLs to \"URL\"\n",
      "\n",
      "\n",
      "X_sents = [nltk.sent_tokenize(line) for line in X_strings]    # list of sentences; used as input for word_tokenize()\n",
      "\n",
      "stopwords = nltk.corpus.stopwords.words('english')    # list of stopwords to be removed\n",
      "punct = string.punctuation    # list of punctuation to be removed\n",
      "\n",
      "X_normalized = []  \n",
      "X_not_normalized = []\n",
      "for question in X_sents:\n",
      "    X_normalized.append([])    # add list container for each question (line of file)\n",
      "    X_not_normalized.append([])\n",
      "    for sent in question:\n",
      "        tokens = nltk.word_tokenize(sent)\n",
      "        normalized = [word for word in tokens if word not in stopwords and word not in punct]\n",
      "        if len(normalized) > 0:    # don't include sentences where every token has been stripped\n",
      "            X_normalized[-1].append(normalized)    # add sentence to question container\n",
      "        X_not_normalized[-1].append(tokens)\n",
      "\n",
      "print X_normalized[0]    # test output\n",
      "print X_not_normalized[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[['yawns', 'contagious'], ['people', 'yawn', 'see', 'people', 'room', 'yawn']]\n",
        "[['why', 'are', 'yawns', 'contagious', '?'], ['when', 'people', 'yawn', ',', 'you', 'see', 'that', 'other', 'people', 'in', 'the', 'room', 'yawn', ',', 'too', '.'], ['why', 'is', 'that', '?']]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Text Normalization\n",
      "1. Remove Stop Words and punctuation (already completed above)\n",
      "2. Special Vocabulary: (years to 'DATE'), (proper nouns to 'NAMES'), (acronym to 'AAA'), (urls to 'URL') ... \n",
      "3. Remove \"&#xd\"... from words (already completed above)\n",
      "4. Correct misspelled words\n",
      "5. Lemmatize\n",
      "6. WordNet"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wnl = nltk.WordNetLemmatizer()\n",
      "for question in X_normalized:\n",
      "    for sent in question:\n",
      "        for word in sent:\n",
      "            if re.match(r\"[12]\\d{3}\", word):    # match years\n",
      "                pass\n",
      "            elif re.match(r\"\", word):    # match URLs\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pyenchant package documentation: http://pythonhosted.org/pyenchant/tutorial.html\n",
      "# still need to install pyenchant: pip install pyenchant\n",
      "d = enchant.Dict(\"en_US\")\n",
      "misspelled_words = [word for word in sent for sent in question for question in X_tokens if not d.check(word)]    # list of misspelled words\n",
      "\n",
      "# this list may be a mix of slangs and names; create names corpus (people, companies, locations) nltk.corpus.names\n",
      "# POS tags - tags of NONE or PROPER NOUN\n",
      "# named entity chunking"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Feature Ideas\n",
      "1. len of question\n",
      "2. POS structure\n",
      "3. \\# of unique letters in word (sentence aggregate average)\n",
      "4. mention of people (proper nouns) - companies, locations\n",
      "5. presence of a number/url/slang/\n",
      "6. wordnet hypernyms for all nouns"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}