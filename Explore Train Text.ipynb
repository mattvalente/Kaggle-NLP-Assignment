{
 "metadata": {
  "name": "",
  "signature": "sha256:8824c23f2236f27b13d477e7d2bc9457cca760974a6fddc6d8ef230dccbdc4e0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, string, collections, csv, numpy, codecs\n",
      "from nltk.corpus import wordnet as wn\n",
      "from sklearn import svm, cross_validation\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction import DictVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 422
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SVM"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_in_text(filepath, filename, csv_file=False):\n",
      "    with codecs.open(filepath + filename, encoding='utf-8') as f:\n",
      "        if csv_file == False:\n",
      "            tups = [(line[0], line[2:].strip()) for line in f]\n",
      "        else:\n",
      "            reader = csv.reader(f)\n",
      "            next(reader, None)  # skip header\n",
      "            tups = [tuple(row) for row in reader]\n",
      "    y, X = [list(t) for t in zip(*tups)]\n",
      "    return X, y         # for test.csv file, y is IDs not class values\n",
      "\n",
      "\n",
      "def mappings(untokenized):\n",
      "    '''Perform on list of text strings'''\n",
      "    x = [re.sub(r\"&#(\\S*?;)+\", \" \", question) for question in untokenized] # strips weird formatting characters (e.g. \"&#xd;&lt;br&gt;\")\n",
      "    x = [re.sub(r\"(http:)?(www.)?\\w*?(.com|.gov|.edu)\", \" URL \", question) for question in x] # map URLs to \"URL\"\n",
      "    x = [re.sub(r\"[12]\\d{3}\", \"YEAR\", question) for question in x]    # map years to \"YEAR\"\n",
      "    return x\n",
      "\n",
      "\n",
      "def hypernyms(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of hypernyms for normalized words\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()  \n",
      "    stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "    punct = string.punctuation                                      # list of punctuation to be removed\n",
      "    hyps = []           \n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    for question in sents:\n",
      "        hyps.append([])     # new list for each question\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            normal = [wnl.lemmatize(word) for word in tokens if word not in stopwords and word not in punct]\n",
      "            for word in normal:\n",
      "                if len(normal) > 0:\n",
      "                    terms = []\n",
      "                    s = wn.synsets(word, 'n')\n",
      "                    for syn in s:\n",
      "                        for h in syn.hypernyms():\n",
      "                            terms = terms + [h.name()]\n",
      "                        hyps[-1].extend(terms)\n",
      "    return hyps\n",
      "\n",
      "\n",
      "def pos_tags(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of pos tagged tokens\n",
      "        Each question is a list of pos tagged sentences\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]    # list of sentences; used as input for word_tokenize()\n",
      "    punct = string.punctuation    # list of punctuation to be removed\n",
      "    tags = []\n",
      "    for question in sents:\n",
      "        tag.append([])\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            tag[-1].append(nltk.pos_tag(tokens))\n",
      "    return tags\n",
      "\n",
      "\n",
      "def classify_hypernyms_svm(hypernyms, y):\n",
      "    strings = [' '.join(question) for question in hypernyms]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    clf.fit(X,y)\n",
      "    return clf, vectorizer\n",
      "\n",
      "def cross_validate(X, y):\n",
      "    strings = [' '.join(x) for x in X]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "    clf.fit(X_train, y_train)\n",
      "    #for x in X_test:\n",
      "    #    print clf.predict(x)\n",
      "    return clf.score(X_test, y_test) \n",
      "\n",
      "def predict(filepath, filename, classifier, vectorizer):\n",
      "    strings, IDs = read_in_text(filepath, filename, csv_file=True)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    s = [' '.join(question) for question in hyps]\n",
      "    X = vectorizer.transform(s)   # use transform instead of fit_transform to ensure matching dimensionality\n",
      "    y_pred = [classifier.predict(x)[0] for x in X]\n",
      "    return y_pred, IDs\n",
      "        \n",
      "\n",
      "def train_algorithm1(filepath, filename):\n",
      "    strings, y = read_in_text(filepath, filename)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    classifier, vectorizer = classify_hypernyms_svm(hyps, y)\n",
      "    print 'cross_validate'\n",
      "    print cross_validate(hyps,y)\n",
      "    return classifier, vectorizer\n",
      "\n",
      "def output(y_pred, IDs):\n",
      "    tups = zip(IDs, y_pred)\n",
      "    tups.insert(0, (\"ID\", \"Category\"))\n",
      "    with open(\"output.csv\", \"w\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        for tup in tups:\n",
      "            writer.writerow(tup)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 423
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filepath = '/Users/matthewvalente/Documents/ischool/fall14/Info256_NLP/Kaggle-NLP-Assignment/'\n",
      "filepath = '/Users/LaContra/Dropbox/Workspace/Kaggle-NLP-Assignment/'\n",
      "train_file = 'train.txt'\n",
      "test_file = 'test.csv'\n",
      "classifier, vectorizer = train_algorithm1(filepath, train_file)\n",
      "\n",
      "y_pred, IDs = predict(filepath, test_file, classifier, vectorizer)\n",
      "output(y_pred, IDs)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "descriptor 'split' requires a 'str' object but received a 'unicode'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-424-646ee12355f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_algorithm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-423-be12ee606c96>\u001b[0m in \u001b[0;36mtrain_algorithm1\u001b[0;34m(filepath, filename)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mnew_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_hypernyms_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'cross_validate'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-423-be12ee606c96>\u001b[0m in \u001b[0;36mclassify_hypernyms_svm\u001b[0;34m(hypernyms, y)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mstrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n\u001b[1;32m     68\u001b[0m     \u001b[0mintercept_scaling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ovr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/LaContra/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 817\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/LaContra/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: descriptor 'split' requires a 'str' object but received a 'unicode'"
       ]
      }
     ],
     "prompt_number": 424
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Na\u00efve Bayes Classifier"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def word_similarity(data):\n",
      "    similarity_map = {}    \n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    stopwords = nltk.corpus.stopwords.words('english') \n",
      "    predefined_groups = [['Business', 'Finance'], ['Computers', 'Internet'], \\\n",
      "                         ['Entertainment', 'Music'], ['Family', 'Relationships'],\\\n",
      "                         ['Education', 'Reference'], ['Health'], ['Science', 'Mathematics']]\n",
      "    \n",
      "    for idx, question in enumerate(data):\n",
      "        strings = ''.join(mappings(question.lower()))\n",
      "        words = [wnl.lemmatize(word) for word in nltk.word_tokenize(strings) if word not in stopwords and word not in string.punctuation]\n",
      "        similarity = []\n",
      "        for groups in predefined_groups:\n",
      "            group_similarity = []\n",
      "            for word in words:\n",
      "                word_syns = wn.synsets(word, 'n')\n",
      "                for group in groups:\n",
      "                    group_syns = wn.synsets(group.lower(), 'n')\n",
      "                    if len(word_syns) > 0 and len(group_syns) > 0:\n",
      "                        group_similarity.append(word_syns[0].path_similarity(group_syns[0]))\n",
      "            similarity.append(numpy.mean(group_similarity))\n",
      "        similarity_map[idx] = similarity\n",
      "        \n",
      "    return similarity_map  \n",
      "\n",
      "def feature_extraction_func(data, idx):\n",
      "    features = {}\n",
      "    \n",
      "    # Len of question(accuracy: 0.24)\n",
      "#     features['len_of_question'] = len(data)\n",
      "    \n",
      "    # Similarity between words and predefined_gorup(accuracy: 0.33)\n",
      "    # Business&Finance, Computers&Internet, Entertainment&Music, Family&Relationships\n",
      "    # Education&Reference, Health, Science&Mathematics\n",
      "    global similarity_map\n",
      "    features['similarity_group'] = similarity_map[idx].index(max(similarity_map[idx]))\n",
      "\n",
      "    # Hypernyms of each word\n",
      "    \n",
      "    \n",
      "    return features\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 407
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def create_train_test_sets(feature_function, questions_data, classes_data, train_idxs, test_idxs):\n",
      "    ques_train_set = [feature_function(item, i) for i, item in enumerate(questions_data) if i in train_idxs]\n",
      "    ques_test_set = [feature_function(item, i) for i, item in enumerate(questions_data) if i in test_idxs]\n",
      "    class_train_set = [item for i, item in enumerate(classes_data) if i in train_idxs]\n",
      "    class_test_set = [item for i, item in enumerate(classes_data) if i in test_idxs]\n",
      "    \n",
      "    return ques_train_set, ques_test_set, class_train_set, class_test_set\n",
      "\n",
      "def crossValidation(questions_data, classes_data, number_folds):\n",
      "    train_test_idxs = cross_validation.KFold(len(questions_data), n_folds=number_folds, shuffle=True, random_state=None)\n",
      "    \n",
      "    # Run Classifier on each fold\n",
      "    accuracy_rates = {}\n",
      "    accuracy_avg_rages = {}\n",
      "    accuracy_rates['NB'] = []\n",
      "    for train_idxs, test_idxs in train_test_idxs:  \n",
      "        ques_train_set, ques_test_set, class_train_set, class_test_set \\\n",
      "            = create_train_test_sets(feature_extraction_func, questions_data, classes_data, train_idxs, test_idxs)\n",
      "\n",
      "        # Na\u00efve Bayes Classfier\n",
      "        nb_classifier = nltk.NaiveBayesClassifier.train(zip(ques_train_set, class_train_set))\n",
      "        accuracy_rates['NB'].append(nltk.classify.accuracy(nb_classifier, zip(ques_test_set, class_test_set)))\n",
      "    \n",
      "        # MaxEnt Classifer\n",
      "        \n",
      "        # Decision Tree\n",
      "        \n",
      "        \n",
      "#     print accuracy_rates['NB']\n",
      "    accuracy_avg_rages['NB'] = numpy.mean(accuracy_rates['NB'])\n",
      "    \n",
      "    return accuracy_avg_rages"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 409
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# train_file2 = 'train2.txt'\n",
      "questions, classes = read_in_text(filepath, train_file)\n",
      "similarity_map = word_similarity(questions)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accuracy = crossValidation(questions, classes, 10)\n",
      "print accuracy "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.35555555555555557, 0.3111111111111111, 0.3111111111111111, 0.3074074074074074, 0.3148148148148148, 0.2851851851851852, 0.32592592592592595, 0.3, 0.3420074349442379, 0.30855018587360594]\n",
        "{'NB': 0.31616687319289544}\n"
       ]
      }
     ],
     "prompt_number": 410
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}