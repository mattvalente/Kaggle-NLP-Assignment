{
 "metadata": {
  "name": "",
  "signature": "sha256:385e225a6f7e3657e68dcde1236927ce0e8feefa03dda2ffc78621228330ccc7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, string, collections, csv, numpy, codecs\n",
      "from nltk.corpus import wordnet as wn\n",
      "from sklearn import svm, cross_validation\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 146
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_in_text(filepath, filename, csv_file=False):\n",
      "    with codecs.open(filepath + filename, encoding='utf-8') as f:\n",
      "        if csv_file == False:\n",
      "            tups = [(line[0], line[2:].strip()) for line in f]\n",
      "        else:\n",
      "            reader = csv.reader(f)\n",
      "            next(reader, None)  # skip header\n",
      "            tups = [tuple(row) for row in reader]\n",
      "    y, X = [list(t) for t in zip(*tups)]\n",
      "    return X, y         # for test.csv file, y is IDs not class values\n",
      "\n",
      "\n",
      "def mappings(untokenized):\n",
      "    '''Perform on list of text strings'''\n",
      "    x = [re.sub(r\"&#(\\S*?;)+\", \" \", question) for question in untokenized] # strips weird formatting characters (e.g. \"&#xd;&lt;br&gt;\")\n",
      "    x = [re.sub(r\"(http:)?(www.)?\\w*?(.com|.gov|.edu)\", \" URL \", question) for question in x] # map URLs to \"URL\"\n",
      "    x = [re.sub(r\"[12]\\d{3}\", \"YEAR\", question) for question in x]    # map years to \"YEAR\"\n",
      "    return x\n",
      "\n",
      "\n",
      "def hypernyms(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of hypernyms for normalized words\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()  \n",
      "    stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "    punct = string.punctuation                                      # list of punctuation to be removed\n",
      "    hyps = []           \n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    for question in sents:\n",
      "        hyps.append([])     # new list for each question\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            normal = [wnl.lemmatize(word) for word in tokens if word not in stopwords and word not in punct]\n",
      "            for word in normal:\n",
      "                if len(normal) > 0:\n",
      "                    terms = []\n",
      "                    s = wn.synsets(word, 'n')\n",
      "                    for syn in s:\n",
      "                        for h in syn.hypernyms():\n",
      "                            terms = terms + [h.name]\n",
      "                        hyps[-1].extend(terms)\n",
      "    return hyps\n",
      "\n",
      "\n",
      "def pos_tags(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of pos tagged tokens\n",
      "        Each question is a list of pos tagged sentences\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]    # list of sentences; used as input for word_tokenize()\n",
      "    punct = string.punctuation    # list of punctuation to be removed\n",
      "    tags = []\n",
      "    for question in sents:\n",
      "        tag.append([])\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            tag[-1].append(nltk.pos_tag(tokens))\n",
      "    return tags\n",
      "\n",
      "\n",
      "def classify_hypernyms_svm(hypernyms, y):\n",
      "    strings = ''\n",
      "    strings = [' '.join(question) for question in hypernyms]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    clf.fit(X,y)\n",
      "    return clf, vectorizer\n",
      "\n",
      "def cross_validate(X, y):\n",
      "    strings = [' '.join(x) for x in X]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "    clf.fit(X_train, y_train)\n",
      "    #for x in X_test:\n",
      "    #    print clf.predict(x)\n",
      "    return clf.score(X_test, y_test) \n",
      "\n",
      "def predict(filepath, filename, classifier, vectorizer):\n",
      "    strings, IDs = read_in_text(filepath, filename, csv_file=True)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    s = [' '.join(question) for question in hyps]\n",
      "    X = vectorizer.transform(s)   # use transform instead of fit_transform to ensure matching dimensionality\n",
      "    y_pred = [classifier.predict(x)[0] for x in X]\n",
      "    return y_pred, IDs\n",
      "        \n",
      "\n",
      "def train_algorithm1(filepath, filename):\n",
      "    strings, y = read_in_text(filepath, filename)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    classifier, vectorizer = classify_hypernyms_svm(hyps, y)\n",
      "    print 'cross_validate'\n",
      "    print cross_validate(hyps,y)\n",
      "    return classifier, vectorizer\n",
      "\n",
      "def output(y_pred, IDs):\n",
      "    tups = zip(IDs, y_pred)\n",
      "    tups.insert(0, (\"ID\", \"Category\"))\n",
      "    with open(\"output.csv\", \"w\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        for tup in tups:\n",
      "            writer.writerow(tup)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-191-14063d78c5bd>, line 70)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-191-14063d78c5bd>\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    strings = [' '.join(str(question) for question in hypernyms]\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# filepath = '/Users/matthewvalente/Documents/ischool/fall14/Info256_NLP/Kaggle-NLP-Assignment/'\n",
      "filepath = '/Users/LaContra/Dropbox/Workspace/Kaggle-NLP-Assignment/'\n",
      "train_file = 'train.txt'\n",
      "test_file = 'test.csv'\n",
      "classifier, vectorizer = train_algorithm1(filepath, train_file)\n",
      "\n",
      "y_pred, IDs = predict(filepath, test_file, classifier, vectorizer)\n",
      "output(y_pred, IDs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "sequence item 0: expected string, instancemethod found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-192-646ee12355f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_algorithm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-189-6e64c15b4ad8>\u001b[0m in \u001b[0;36mtrain_algorithm1\u001b[0;34m(filepath, filename)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mnew_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmappings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mhyps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_hypernyms_svm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'cross_validate'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-189-6e64c15b4ad8>\u001b[0m in \u001b[0;36mclassify_hypernyms_svm\u001b[0;34m(hypernyms, y)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#             print question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mstrings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhypernyms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected string, instancemethod found"
       ]
      }
     ],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def feature_extraction_func(data):\n",
      "    features = {}\n",
      "\n",
      "    # Len of question\n",
      "    features['len_of_question'] = len(data)\n",
      "\n",
      "    # Presence of a #/url/slang\n",
      "\n",
      "    # Hypernyms of each word\n",
      "\n",
      "    return features\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_training_sets(feature_function, data):\n",
      "    return zip([feature_function(key) for key in data[0]], data[1])\n",
      "\n",
      "def crossValidation(data, number_folds):\n",
      "    feature_set = create_training_sets(feature_extraction_func, data)\n",
      "    train_test_idxs = cross_validation.KFold(len(feature_set), n_folds=number_folds, shuffle=True, random_state=None)\n",
      "\n",
      "    # Run Classifier on each fold\n",
      "    accuracy_rates = {}\n",
      "    accuracy_avg_rages = {}\n",
      "    accuracy_rates['NB'] = []\n",
      "    for train_idxs, test_idxs in train_test_idxs:   \n",
      "        # SVM Classifier\n",
      "        \n",
      "        # Na\u00efve Bayes Classfier\n",
      "        train_set = [item for i, item in enumerate(feature_set) if i in train_idxs]\n",
      "        test_set = [item for i, item in enumerate(feature_set) if i in test_idxs]\n",
      "        \n",
      "        nb_classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
      "        accuracy_rates['NB'].append(nltk.classify.accuracy(nb_classifier, test_set))\n",
      "    \n",
      "        # MaxEnt Classifer\n",
      "        \n",
      "        # Decision Tree\n",
      "        \n",
      "        \n",
      "    print accuracy_rates['NB']\n",
      "    accuracy_avg_rages['NB'] = numpy.mean(accuracy_rates['NB'])\n",
      "    return accuracy_avg_rages\n",
      "      \n",
      "# train_file2 = 'train2.txt'\n",
      "accuracy = crossValidation(read_in_text(filepath, train_file), 10)\n",
      "print accuracy \n",
      "\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0.26296296296296295, 0.27037037037037037, 0.25925925925925924, 0.2851851851851852, 0.26666666666666666, 0.1925925925925926, 0.2074074074074074, 0.26296296296296295, 0.24907063197026022, 0.22304832713754646]\n",
        "{'NB': 0.2479526366515214}\n"
       ]
      }
     ],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}