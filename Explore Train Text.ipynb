{
 "metadata": {
  "name": "",
  "signature": "sha256:f73c4a9f1a1eeb570e32f5f57bc83694c0ca6d149dbf6af87bd33105d64d3f03"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk, re, string, collections, csv\n",
      "from nltk.corpus import wordnet as wn\n",
      "from sklearn import svm, cross_validation\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_in_text(filepath, filename, csv_file=False):\n",
      "    with open(filepath + filename) as f:\n",
      "        if csv_file == False:\n",
      "            tups = [(line[0], line[2:].strip()) for line in f]\n",
      "        else:\n",
      "            reader = csv.reader(f)\n",
      "            next(reader, None)  # skip header\n",
      "            tups = [tuple(row) for row in reader]\n",
      "    y, X = [list(t) for t in zip(*tups)]\n",
      "    return X, y         # for test.csv file, y is IDs not class values\n",
      "\n",
      "\n",
      "def mappings(untokenized):\n",
      "    '''Perform on list of text strings'''\n",
      "    x = [re.sub(r\"&#(\\S*?;)+\", \" \", question) for question in untokenized] # strips weird formatting characters (e.g. \"&#xd;&lt;br&gt;\")\n",
      "    x = [re.sub(r\"(http:)?(www.)?\\w*?(.com|.gov|.edu)\", \" URL \", question) for question in untokenized] # map URLs to \"URL\"\n",
      "    x = [re.sub(r\"[12]\\d{3}\", \"YEAR\", question) for question in untokenized]    # map years to \"YEAR\"\n",
      "    return x\n",
      "\n",
      "\n",
      "def hypernyms(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of hypernyms for normalized words\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]      # list of sentences; used as input for word_tokenize()\n",
      "    stopwords = nltk.corpus.stopwords.words('english')              # list of stopwords to be removed\n",
      "    punct = string.punctuation                                      # list of punctuation to be removed\n",
      "    hyps = []           \n",
      "    wnl = nltk.WordNetLemmatizer()\n",
      "    for question in sents:\n",
      "        hyps.append([])     # new list for each question\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            normal = [wnl.lemmatize(word) for word in tokens if word not in stopwords and word not in punct]\n",
      "            for word in normal:\n",
      "                if len(normal) > 0:\n",
      "                    terms = []\n",
      "                    s = wn.synsets(word, 'n')\n",
      "                    for syn in s:\n",
      "                        for h in syn.hypernyms():\n",
      "                            terms = terms + [h.name]\n",
      "                        hyps[-1].extend(terms)\n",
      "    return hyps\n",
      "\n",
      "\n",
      "def pos_tags(untokenized):\n",
      "    ''' Takes untokenized list of questions and returns\n",
      "        a list of pos tagged tokens\n",
      "        Each question is a list of pos tagged sentences\n",
      "    '''\n",
      "    sents = [nltk.sent_tokenize(line) for line in untokenized]    # list of sentences; used as input for word_tokenize()\n",
      "    punct = string.punctuation    # list of punctuation to be removed\n",
      "    tags = []\n",
      "    for question in sents:\n",
      "        tag.append([])\n",
      "        for sent in question:\n",
      "            tokens = nltk.word_tokenize(sent)\n",
      "            tag[-1].append(nltk.pos_tag(tokens))\n",
      "    return tags\n",
      "\n",
      "\n",
      "def classify_hypernyms_svm(hypernyms, y):\n",
      "    strings = [' '.join(question) for question in hypernyms]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    clf.fit(X,y)\n",
      "    return clf, vectorizer\n",
      "\n",
      "def cross_validate(X, y):\n",
      "    strings = [' '.join(x) for x in X]\n",
      "    vectorizer = CountVectorizer(analyzer=str.split)\n",
      "    X = vectorizer.fit_transform(strings)\n",
      "    clf = svm.LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "    intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',\n",
      "    random_state=None, tol=0.0001, verbose=0) \n",
      "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=0.2, random_state=0)\n",
      "    clf.fit(X_train, y_train)\n",
      "    #for x in X_test:\n",
      "    #    print clf.predict(x)\n",
      "    return clf.score(X_test, y_test) \n",
      "\n",
      "def predict(filepath, filename, classifier, vectorizer):\n",
      "    strings, IDs = read_in_text(filepath, filename, csv_file=True)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    s = [' '.join(question) for question in hyps]\n",
      "    X = vectorizer.transform(s)   # use transform instead of fit_transform to ensure matching dimensionality\n",
      "    y_pred = [classifier.predict(x)[0] for x in X]\n",
      "    return y_pred, IDs\n",
      "        \n",
      "\n",
      "def train_algorithm1(filepath, filename):\n",
      "    strings, y = read_in_text(filepath, filename)\n",
      "    new_strings = mappings(strings)\n",
      "    hyps = hypernyms(new_strings)\n",
      "    classifier, vectorizer = classify_hypernyms_svm(hyps, y)\n",
      "    print cross_validate(hyps,y)\n",
      "    return classifier, vectorizer\n",
      "\n",
      "def output(y_pred, IDs):\n",
      "    tups = zip(IDs, y_pred)\n",
      "    tups.insert(0, (\"ID\", \"Category\"))\n",
      "    with open(\"output.csv\", \"w\") as f:\n",
      "        writer = csv.writer(f)\n",
      "        for tup in tups:\n",
      "            writer.writerow(tup)\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filepath = '/Users/matthewvalente/Documents/ischool/fall14/Info256_NLP/Kaggle-NLP-Assignment/'\n",
      "train_file = 'train.txt'\n",
      "test_file = 'test.csv'\n",
      "classifier, vectorizer = train_algorithm1(filepath, train_file)\n",
      "\n",
      "y_pred, IDs = predict(filepath, test_file, classifier, vectorizer)\n",
      "output(y_pred, IDs)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
